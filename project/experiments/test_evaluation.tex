\subsection{Test set evaluation} \label{test_evaluation}
The models chosen for the test set evaluation come from the multiloss experiments. Based on \textit{Setup B}, they have the following configuration:
\begin{itemize}
    \item \textbf{KB modes:} Bottom Up and Top Down
    \item \textbf{initial clause weights:} 0.5 and variable
    \item \textbf{learnable clause weights}
    \item \textbf{encoder:} BERT with adapters
    \item \textbf{loss function:} multiloss with $\alpha = 0.5$
\end{itemize}
% scelta modelli
The choice of the multiloss setup has the following reasons derived from the experimental results:
\begin{enumerate}
    \item When using a standard loss setup, the pre-KENN network adapts its predictions to the presence of KENN and achieves results that are very similar to the baseline ones. This means that KENN-based models can be seen as simple baseline models that have had different evolutions due to the noise introduced by KENN.
    \item When using a multiloss setup, the pre-KENN network avoids the adaptation thanks to the pre-KENN loss. The final clause weights will ensure that the network only uses KENN when it really needs it.
    \item The choice of variable clause weights may emphasize the effect of the multiloss on final weights, thus producing models in which the influence of KENN is slightly different.
\end{enumerate}
The test set has been evaluated using the metrics of \textit{macro f1 examples} and \textit{macro f1 classes}. The inference rules used to compute the final predictions are the \textit{threshold} and \textit{threshold or max}.

\paragraphn{Results on FIGER}
Table~\ref{tab:performance_figer} shows the results obtained on FIGER. If we look at the performance with the \textit{threshold} inference, we can see that the Top Down mode is the best in every metric. With respect to the baseline model, it gains 0.0151 on \textit{macro f1 examples} and 0.0108 on \textit{macro f1 classes}. 

KENN-based models become even more competitive when using the \textit{threshold or max} inference. In this case, the metrics obtained by Bottom Up and Top Down are quite similar. The best performance on \textit{macro f1 examples} and \textit{macro f1 classes} are reached by Bottom Up and Top Down by gaining 0.0181 and 0.0137, respectively, over the baseline. Note that by changing the inference rule, Bottom Up and Top Down increase their \textit{macro examples f1} much more than the baseline.

\begin{table}[H]
\centering
\caption{Performance of the baseline and KENN multiloss models on the test set - FIGER}
\label{tab:performance_figer}
\resizebox{\columnwidth}{!}{\begin{tabular}{cc|ccc|ccc|}
\cline{3-8}
\textbf{} &  & \multicolumn{3}{c|}{\textbf{Macro examples}} & \multicolumn{3}{c|}{\textbf{Macro classes}} \\ \hline
\multicolumn{1}{|c|}{\textbf{KB mode}} & \textbf{Inference} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} \\ \hline
\noalign{\hrule height 1pt}
\multicolumn{1}{|c|}{- (baseline)} & threshold & 0.7884 & 0.8601 & 0.8227 & 0.2305 & 0.2526 & 0.2410 \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Bottom Up\\ (variable weights)\end{tabular}} & threshold & 0.7631 & 0.8501 & 0.8043 & 0.2171 & 0.2459 & 0.2306 \\ \hline
\multicolumn{1}{|c|}{Bottom Up} & threshold & 0.7854 & 0.8611 & 0.8215 & 0.2343 & 0.2630 & 0.2478 \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Top Down\\ (variable weights)\end{tabular}} & threshold & 0.7560 & 0.8627 & 0.8058 & 0.2157 & 0.2445 & 0.2292 \\ \hline
\multicolumn{1}{|c|}{Top Down} & threshold & \textbf{0.7997} & \textbf{0.8796} & \textbf{0.8378} & \textbf{0.2404} & \textbf{0.2643} & \textbf{0.2518} \\ \hline
\noalign{\hrule height 1pt}
\multicolumn{1}{|c|}{- (baseline)} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & 0.8008 & 0.8699 & 0.8339 & 0.2355 & 0.2542 & 0.2445 \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Bottom Up\\ (variable weights)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & 0.7950 & 0.8795 & 0.8351 & 0.2156 & 0.2492 & 0.2312 \\ \hline
\multicolumn{1}{|c|}{Bottom Up} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & \textbf{0.8174} & 0.8898 & \textbf{0.8520} & 0.2327 & \textbf{0.2718} & 0.2507 \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Top Down\\ (variable weights)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & 0.7773 & 0.8831 & 0.8268 & 0.2298 & 0.2518 & 0.2403 \\ \hline
\multicolumn{1}{|c|}{Top Down} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & 0.8139 & \textbf{0.8903} & 0.8504 & \textbf{0.2495} & 0.2676 & \textbf{0.2582} \\ \hline
\end{tabular}}
\end{table}



\paragraphn{Results on BBN}
% threshold: top down i due migliori
In Table~\ref{tab:performance_bbn} we can find the results obtained on BBN. Except for the \textit{macro classes} metrics, the best results are achieved by both the Top Down modes regardless of the inference rule. This time, in constrast to FIGER, the baseline and KENN-based models have a similar boost on the performance when moving from \textit{threshold} to \textit{threshold or max} inference.

\begin{table}[H]
\centering
\caption{Performance of the baseline and KENN multiloss models on the test set - BBN}
\label{tab:performance_bbn}
\resizebox{\columnwidth}{!}{\begin{tabular}{cc|ccc|ccc|}
\cline{3-8}
\textbf{} &  & \multicolumn{3}{c|}{\textbf{Macro examples}} & \multicolumn{3}{c|}{\textbf{Macro classes}} \\ \hline
\multicolumn{1}{|c|}{\textbf{KB mode}} & \textbf{Inference} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} \\ \hline
\noalign{\hrule height 1pt}
\multicolumn{1}{|c|}{- (baseline)} & threshold & 0.7428 & 0.8197 & 0.7793 & \textbf{0.4893} & 0.4981 & \textbf{0.4937} \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Bottom Up\\ (variable weights)\end{tabular}} & threshold & 0.7489 & 0.8222 & 0.7839 & 0.4783 & 0.4947 & 0.4864 \\ \hline
\multicolumn{1}{|c|}{Bottom Up} & threshold & 0.7417 & 0.8149 & 0.7766 & 0.4682 & \textbf{0.5099} & 0.4882 \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Top Down\\ (variable weights)\end{tabular}} & threshold & 0.7540 & 0.8279 & 0.7892 & 0.4825 & 0.4990 & 0.4906 \\ \hline
\multicolumn{1}{|c|}{Top Down} & threshold & \textbf{0.7551} & \textbf{0.8297} & \textbf{0.7906} & 0.4560 & 0.4770 & 0.4663 \\ \hline
\noalign{\hrule height 1pt}
\multicolumn{1}{|c|}{- (baseline)} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & 0.7531 & 0.8272 & 0.7884 & 0.4730 & 0.5007 & 0.4865 \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Bottom Up\\ (variable weights)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & 0.7617 & 0.8305 & 0.7946 & 0.4607 & 0.5019 & 0.4804 \\ \hline
\multicolumn{1}{|c|}{Bottom Up} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & 0.7516 & 0.8216 & 0.7851 & 0.4589 & 0.5125 & 0.4842 \\ \hline
\multicolumn{1}{|c|}{\begin{tabular}[c]{@{}c@{}}Top Down\\ (variable weights)\end{tabular}} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & 0.7649 & 0.8353 & 0.7986 & \textbf{0.4775} & \textbf{0.5131} & \textbf{0.4947} \\ \hline
\multicolumn{1}{|c|}{Top Down} & \begin{tabular}[c]{@{}c@{}}threshold\\ or max\end{tabular} & \textbf{0.7676} & \textbf{0.8387} & \textbf{0.8016} & 0.4477 & 0.4917 & 0.4687 \\ \hline
\end{tabular}}
\end{table}

\paragraphn{Test set considerations}
% 18/24 le modalitÃ  top down sono andate meglio -> motivi? la rete fa meno fatica a classificare i tipi coarse, proababile sia quello
The configurations involving the Top Down mode are the ones that achieved the best results. If we group the results of both datasets by the inference rule and consider the performance of Top Down with and without variable weights together, we can see that this KB mode reached the highest metric in 18 out of 24 cases\footnote{$|metrics| \times |inference \; rules| \times |datasets| = 6 \times 2 \times 2 = 24$ metrics}. The reason for these results may be that is known that the classification of a supertype (i.e., the antecedent of a Top Down clause) is generally easier than the classification of a subtype, so the boost produced by KENN seems to be more effective when propagated from supertypes to subtypes. However, this aspect needs to be further investigated and has been left for future work.

A final consideration can be made on the test sets as they have significant differences compared to the training sets (section~\ref{dataset_stats}). The noise that characterizes a dataset generated by distant supervision techniques could limit the potential of a model. By inspecting some automatically annotated entries from the training sets, we can find several mentions labeled with completely different types even if they appear in very similar contexts. Conversely, this kind of entries are not present in the test sets. This means that a model trained on this noisy data could have difficulties to learn some types. Regarding KENN, this noise could be even more confusing due to the presence of logical clauses, which have an important role in the learning process. If we think about the multiloss experiments, for example, a cleaner dataset could have led to a completely different evolution of clause weights, thus leading to a different influence of KENN at inference time.